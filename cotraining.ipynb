{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UN7UavpVfqVc"
   },
   "source": [
    "# co-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "abGNtimgfqVh"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xWEikDnHfqVl"
   },
   "outputs": [],
   "source": [
    "from math import floor\n",
    "import numpy as np\n",
    "import copy\n",
    "import random\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "LbC0oOpWfqVs"
   },
   "outputs": [],
   "source": [
    "def train(loader, model, loss_fn, optimizer, device):\n",
    "    size = len(loader.dataset)\n",
    "    model.train()\n",
    "    i = 0\n",
    "    for batch, (X, y) in enumerate(loader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss, current = loss, (batch + 1) * len(X)\n",
    "        print(f\"loss: {loss:>7f} [{current:5d} / {size:>5d}]\")\n",
    "        i += 1\n",
    "        \n",
    "        # if batch % 10 == 0:\n",
    "        #     loss, current = loss, (batch + 1) * len(X)\n",
    "        #     print(f\"loss: {loss:>7f} [{current:5d} / {size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "JICxp7zvZqdU"
   },
   "outputs": [],
   "source": [
    "def test(loader, model, loss_fn, device):\n",
    "  size = len(loader.dataset)\n",
    "  num_batches = len(loader)\n",
    "  model.eval()\n",
    "  test_loss, correct = 0, 0\n",
    "  with torch.no_grad():\n",
    "    for X, y in loader:\n",
    "      X, y = X.to(device), y.to(device)\n",
    "      pred = model(X)\n",
    "      test_loss += loss_fn(pred, y).item()\n",
    "      correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "      \n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}\")\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "TJV3jF3VfqVu"
   },
   "outputs": [],
   "source": [
    "def predict(loader, model, device):\n",
    "    print(f\"Number of instances: {len(loader.dataset)}\")\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for X, y in loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            output = model(X)\n",
    "            predictions.append(output)\n",
    "    return torch.cat(predictions) # output shape (# instances, # outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "TqLhkaSzfqVy"
   },
   "outputs": [],
   "source": [
    "# takes in a Tensor of shape e.g. (# instances, # prob outputs) and returns a tuple\n",
    "# (Tensor[top probabilities], Tensor[predicted labels], Tensor[instance indexes])\n",
    "def get_topk_pred(pred, k):\n",
    "    prob, label = torch.max(pred, 1)\n",
    "    idx = torch.argsort(prob, descending=True)[:k]\n",
    "    return prob[idx].cpu(), label[idx].cpu(), idx.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_imagefolder(paths, labels, dataset):\n",
    "    \"\"\"\n",
    "    Adds the paths with the labels to an image classification dataset\n",
    "\n",
    "    :list paths: a list of absolute image paths to add to the dataset\n",
    "    :list labels: a list of labels for each path\n",
    "    :Dataset dataset: the dataset to add the samples to\n",
    "    \"\"\"\n",
    "\n",
    "    new_samples = list(zip(paths, labels))\n",
    "\n",
    "    dataset.samples += new_samples\n",
    "\n",
    "    return dataset.samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "_lBfOpkufqV0"
   },
   "outputs": [],
   "source": [
    "def remove_collisions(lbl_model0, lbl_model1, idx_model0, idx_model1):\n",
    "    # find instances and indices of instances that have\n",
    "    # been labeled as most confident by both model0, model1\n",
    "    inter, idx_inter0, idx_inter1 = np.intersect1d(\n",
    "                                        idx_model0,\n",
    "                                        idx_model1,\n",
    "                                        return_indices=True)\n",
    "\n",
    "    print(f\"Number of predictions (model0): {len(idx_model0)}\")\n",
    "    print(f\"Number of predictions (model1): {len(idx_model1)}\")\n",
    "    print(f\"Found {len(inter)} potential conflicting predictions\")\n",
    "\n",
    "    # bool mask to identify the conflicting predictions (collision)\n",
    "    mask_coll = lbl_model0[idx_inter0] != lbl_model1[idx_inter1]\n",
    "    collisions = inter[mask_coll]\n",
    "\n",
    "    print(f\"Found {len(collisions)} conflicting predictions\")\n",
    "\n",
    "    if (len(collisions) > 0):\n",
    "        print(f\"Collisions: {collisions}\")\n",
    "        # find where these collisions are actually at\n",
    "        # in their respective lists, and remove them...\n",
    "        # maybe want to return this as well? ...\n",
    "        idx_coll0 = idx_inter0[mask_coll]\n",
    "        idx_coll1 = idx_inter1[mask_coll]\n",
    "\n",
    "        # masks to remove the instances with conflicting predictions\n",
    "        mask0 = np.ones(len(idx_model0), dtype=bool)\n",
    "        mask0[idx_coll0] = False\n",
    "        mask1 = np.ones(len(idx_model1), dtype=bool)\n",
    "        mask1[idx_coll1] = False\n",
    "\n",
    "        lbl_model0 = lbl_model0[mask0]\n",
    "        lbl_model1 = lbl_model1[mask1]\n",
    "        idx_model0 = idx_model0[mask0]\n",
    "        idx_model1 = idx_model1[mask1]\n",
    "\n",
    "    return lbl_model0, lbl_model1, idx_model0, idx_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "fK8GHoO9fqV2"
   },
   "outputs": [],
   "source": [
    "# train two models on two different views\n",
    "# then add top k% of predictions on the unlabeled set\n",
    "# to the labeled datasets\n",
    "def cotrain(loader0, loader1, loader_unlbl0, loader_unlbl1,\n",
    "            model0, model1, k, device):\n",
    "\n",
    "    # make predictions\n",
    "    print(\"Making predictions with model0...\")\n",
    "    pred_model0 = predict(loader_unlbl0, model0, device)\n",
    "    print(\"Making predictions with model1...\")\n",
    "    pred_model1 = predict(loader_unlbl1, model1, device)\n",
    "\n",
    "    # get top-k predictions (labels, instance indexes in the dataset)\n",
    "    print(\"Finding top-k predictions...\")\n",
    "    _, lbl_topk0, idx_topk0 = get_topk_pred(\n",
    "                                    pred_model0,\n",
    "                                    k if k <= len(pred_model0) else len(pred_model0))\n",
    "    _, lbl_topk1, idx_topk1 = get_topk_pred(\n",
    "                                    pred_model1, \n",
    "                                    k if k <= len(pred_model1) else len(pred_model1))\n",
    "\n",
    "    print(f\"Number of unlabeled instances: {len(loader_unlbl0.dataset)}\")\n",
    "\n",
    "    # what if two models predict confidently on the same instance?\n",
    "    # find and remove conflicting predictions from the lists\n",
    "    # may want to return the indices of the collisions too...?\n",
    "    lbl_topk0, lbl_topk1, idx_topk0, idx_topk1 = \\\n",
    "    remove_collisions(lbl_topk0, lbl_topk1, idx_topk0, idx_topk1)\n",
    "\n",
    "    # convert from list to array for the convenient numpy indexing \n",
    "    samples_unlbl0 = np.stack([np.array(a) for a in loader_unlbl0.dataset.samples])\n",
    "    samples_unlbl1 = np.stack([np.array(a) for a in loader_unlbl1.dataset.samples])\n",
    "\n",
    "    print(f\"Shape of samples_unlbl0: {samples_unlbl0.shape}\")\n",
    "    print(f\"last element: {samples_unlbl0[-1][:]}\") \n",
    "\n",
    "    list_samples0 = [(str(a[0]), int(a[1])) for a in list(samples_unlbl0[idx_topk0])]\n",
    "    list_samples1 = [(str(a[0]), int(a[1])) for a in list(samples_unlbl1[idx_topk1])]\n",
    "    \n",
    "    print(f\"Number of elements in list_samples0: {len(list_samples0)}\")\n",
    "    print(f\"last element: {list_samples0[-1]}\")\n",
    "    \n",
    "    paths0 = [i for i, _ in list_samples0]\n",
    "    paths1 = [i for i, _ in list_samples1]\n",
    "\n",
    "    print(f\"Number of elements in paths0: {len(paths0)}\")\n",
    "    print(f\"last element: {paths0[-1]}\")\n",
    "\n",
    "    print(f\"Number of elements in lbl_topk0: {len(lbl_topk0)}\")\n",
    "    print(f\"last element: {lbl_topk0[-1]}\")\n",
    "\n",
    "    # add pseudolabeled instances to the labeled datasets\n",
    "    loader0.dataset.samples = add_to_imagefolder(paths1, lbl_topk1.tolist(), loader0.dataset)\n",
    "    loader1.dataset.samples = add_to_imagefolder(paths0, lbl_topk0.tolist(), loader1.dataset)\n",
    "\n",
    "    print(f\"loader0 number of samples: {len(loader0.dataset.samples)}\")\n",
    "    print(f\"last element: {loader0.dataset.samples[-1]}\")\n",
    "\n",
    "    # remove instances from unlabeled dataset\n",
    "    mask_unlbl0 = np.ones(len(loader_unlbl0.dataset), dtype=bool)\n",
    "    mask_unlbl1 = np.ones(len(loader_unlbl1.dataset), dtype=bool)\n",
    "\n",
    "    mask_unlbl0[idx_topk0] = False\n",
    "    mask_unlbl1[idx_topk1] = False\n",
    "\n",
    "    print(f\"Number of unlabeled instances to remove: {(~mask_unlbl0).sum()}\")\n",
    "\n",
    "    samples_unlbl0 = samples_unlbl0[mask_unlbl0]\n",
    "    samples_unlbl1 = samples_unlbl1[mask_unlbl1]\n",
    "\n",
    "    list_unlbl0 = [(str(a[0]), int(a[1])) for a in list(samples_unlbl0)]\n",
    "    list_unlbl1 = [(str(a[0]), int(a[1])) for a in list(samples_unlbl1)]\n",
    "\n",
    "    loader_unlbl0.dataset.samples = list_unlbl0\n",
    "    loader_unlbl1.dataset.samples = list_unlbl1\n",
    "\n",
    "    print(f\"loader_unlbl0 number of samples: {len(loader_unlbl0.dataset.samples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to split the datasets of the two views so that\n",
    "# the samples in the views are still aligned, time-wise, by index\n",
    "def train_test_split_samples(samples0, samples1, test_size, random_state=None):\n",
    "    if random_state is not None:\n",
    "        random.seed(random_state)\n",
    "\n",
    "    assert test_size > 0 and test_size < 1, \\\n",
    "        'test_size should be a float between (0, 1)'\n",
    "\n",
    "    assert len(samples0) == len(samples1), \\\n",
    "        'number of samples in samples0, samples1 are not equal'\n",
    "    \n",
    "    idx_samples = list(range(len(samples0)))\n",
    "    idx_test = random.sample(idx_samples, floor(test_size * len(samples0)))\n",
    "    idx_train = list(set(idx_samples) - set(idx_test))\n",
    "\n",
    "    # convert to np array for convenient array indexing shenanigans\n",
    "    samples0_np = np.stack([np.array(a) for a in samples0])\n",
    "    samples1_np = np.stack([np.array(a) for a in samples1])\n",
    "    \n",
    "    samples_train0 = [(str(a[0]), int(a[1])) for a in list(samples0_np[idx_train])]\n",
    "    samples_test0 = [(str(a[0]), int(a[1])) for a in list(samples0_np[idx_test])]\n",
    "    samples_train1 = [(str(a[0]), int(a[1])) for a in list(samples1_np[idx_train])]\n",
    "    samples_test1 = [(str(a[0]), int(a[1])) for a in list(samples1_np[idx_test])]\n",
    "\n",
    "    return samples_train0, samples_train1, samples_test0, samples_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_imagefolder(data, samples, path, transform, new_path=None):\n",
    "    imgfolder = datasets.ImageFolder(path, transform=transform)\n",
    "    imgfolder.class_to_idx = data['class_map']\n",
    "    imgfolder.classes = list(data['class_map'].keys())\n",
    "    imgfolder.samples = samples\n",
    "\n",
    "    if new_path is not None:\n",
    "        imgfolder.root = new_path\n",
    "\n",
    "    return imgfolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BMJ6WcfZfqV-",
    "outputId": "8a71f7c1-8ed7-4012-bf07-01e45e7a1efc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('cotraining_samples_lists.pkl', 'rb') as fp:\n",
    "#     dict = pickle.load(fp)\n",
    "with open('cotraining_samples_lists_fixed.pkl', 'rb') as fp:\n",
    "    dict = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['labeled', 'inferred', 'class_map'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dry': 0, 'snow': 1, 'wet': 2}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict['class_map']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('/ourdisk/hpc/ai2es/jroth/data/labeled/bronx_allsites/snow/NYSDOT_uyomtjhwsay_2022-01-29-06-51-02.jpg', 1)\n",
      "('/ourdisk/hpc/ai2es/datasets/DOT/Skyline_6464/20220129/I_87_at_Interchange_3_(Yonkers_Mile_Square_Road)__Northbound__Skyline_6464_2022-01-29-06:50:09.jpg', 1)\n"
     ]
    }
   ],
   "source": [
    "print(dict['labeled'][0])\n",
    "print(dict['inferred'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into labeled/unlabeled\n",
    "# split the data so we get 70/10/20 train/val/test\n",
    "samples_train0, samples_train1, samples_unlbl0, samples_unlbl1 = \\\n",
    "    train_test_split_samples(dict['labeled'], dict['inferred'],\n",
    "                             test_size=0.75, random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3227\n",
      "1076\n"
     ]
    }
   ],
   "source": [
    "print(len(samples_unlbl0))\n",
    "print(len(samples_train0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "754\n",
      "107\n",
      "3227\n",
      "215\n"
     ]
    }
   ],
   "source": [
    "# split the data so we get 70/10/20 train/val/test\n",
    "\n",
    "# split the data so we get 70/10/20 train/val/test\n",
    "samples_train0, samples_train1, samples_test0, samples_test1 = \\\n",
    "        train_test_split_samples(samples_train0, samples_train1,\n",
    "                                 test_size=0.2, random_state=13)\n",
    "\n",
    "samples_train0, samples_train1, samples_val0, samples_val1 = \\\n",
    "        train_test_split_samples(samples_train0, samples_train1,\n",
    "                                 test_size=0.125, random_state=13)\n",
    "\n",
    "print(len(samples_train0))\n",
    "print(len(samples_val0))\n",
    "print(len(samples_unlbl0))\n",
    "print(len(samples_test0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Create ImageFolder objects for first view\n",
    "    dummy_path = '/ourdisk/hpc/ai2es/jroth/data/labeled' \n",
    "    data_train0 = create_imagefolder(dict, samples_train0, dummy_path, trans)\n",
    "    data_unlbl0 = create_imagefolder(dict, samples_unlbl0, dummy_path, trans)\n",
    "    data_val0 = create_imagefolder(dict, samples_val0, dummy_path, trans)\n",
    "    data_test0 = create_imagefolder(dict, samples_test0, dummy_path, trans)\n",
    "\n",
    "    # Create ImageFolder objects for second view (we will also update the root/path)\n",
    "    new_path = '/ourdisk/hpc/ai2es'\n",
    "    data_train1 = create_imagefolder(dict, samples_train1, dummy_path, trans, new_path)\n",
    "    data_unlbl1 = create_imagefolder(dict, samples_unlbl1, dummy_path, trans, new_path)\n",
    "    data_val1 = create_imagefolder(dict, samples_val1, dummy_path, trans, new_path)\n",
    "    data_test1 = create_imagefolder(dict, samples_test1, dummy_path, trans, new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "754\n",
      "3227\n",
      "107\n",
      "215\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "print(len(data_train0.samples))\n",
    "print(len(data_unlbl0.samples))\n",
    "print(len(data_val0.samples))\n",
    "print(len(data_test0.samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_kwargs = {'batch_size': 64, 'shuffle': False}\n",
    "\n",
    "loader_train0 = DataLoader(data_train0, **loader_kwargs)\n",
    "loader_unlbl0 = DataLoader(data_unlbl0, **loader_kwargs)\n",
    "loader_val0 = DataLoader(data_val0, **loader_kwargs)\n",
    "loader_test0 = DataLoader(data_test0, **loader_kwargs)\n",
    "\n",
    "loader_train1 = DataLoader(data_train1, **loader_kwargs)\n",
    "loader_unlbl1 = DataLoader(data_unlbl1, **loader_kwargs)\n",
    "loader_val1 = DataLoader(data_val1, **loader_kwargs)\n",
    "loader_test1 = DataLoader(data_test1, **loader_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "754\n",
      "3227\n",
      "107\n",
      "215\n"
     ]
    }
   ],
   "source": [
    "# sanity checks\n",
    "print(len(loader_train0.dataset))\n",
    "print(len(loader_unlbl0.dataset))\n",
    "print(len(loader_val0.dataset))\n",
    "print(len(loader_test0.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model0, model1 = resnet50().to(device), resnet50().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer0 = torch.optim.SGD(model0.parameters(), lr=1e-3,momentum=0.9)\n",
    "optimizer1 = torch.optim.SGD(model1.parameters(), lr=1e-3, momentum=0.9)\n",
    "\n",
    "# we also need to define some sort of learning rate/early stopping scheduler\n",
    "scheduler0 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer0)\n",
    "scheduler1 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 7.918698 [   64 /   754]\n",
      "loss: 7.163846 [  128 /   754]\n",
      "loss: 5.569634 [  192 /   754]\n",
      "loss: 3.425819 [  256 /   754]\n",
      "loss: 2.244082 [  320 /   754]\n",
      "loss: 1.429785 [  384 /   754]\n",
      "loss: 1.638918 [  448 /   754]\n",
      "loss: 1.192577 [  512 /   754]\n",
      "loss: 1.884185 [  576 /   754]\n",
      "loss: 1.425311 [  640 /   754]\n",
      "loss: 1.524723 [  704 /   754]\n",
      "loss: 1.206422 [  600 /   754]\n",
      "loss: 7.396282 [   64 /   754]\n",
      "loss: 6.639572 [  128 /   754]\n",
      "loss: 4.940058 [  192 /   754]\n",
      "loss: 3.292403 [  256 /   754]\n",
      "loss: 2.244731 [  320 /   754]\n",
      "loss: 1.600625 [  384 /   754]\n",
      "loss: 1.776936 [  448 /   754]\n",
      "loss: 1.153889 [  512 /   754]\n",
      "loss: 1.636461 [  576 /   754]\n",
      "loss: 1.696192 [  640 /   754]\n",
      "loss: 1.559057 [  704 /   754]\n",
      "loss: 1.121512 [  600 /   754]\n"
     ]
    }
   ],
   "source": [
    "train(loader_train0, model0, loss_fn, optimizer0, device)\n",
    "train(loader_train1, model1, loss_fn, optimizer1, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions with model0...\n",
      "Number of instances: 3227\n",
      "Making predictions with model1...\n",
      "Number of instances: 3227\n",
      "Finding top-k predictions...\n",
      "Number of unlabeled instances: 3227\n",
      "Number of predictions (model0): 161\n",
      "Number of predictions (model1): 161\n",
      "Found 2 potential conflicting predictions\n",
      "Found 0 conflicting predictions\n",
      "Shape of samples_unlbl0: (3227, 2)\n",
      "last element: ['/ourdisk/hpc/ai2es/jroth/data/labeled/bronx_allsites/dry/NYSDOT_uyomtjhwsay_2022-02-14-20-31-06.jpg'\n",
      " '0']\n",
      "Number of elements in list_samples0: 161\n",
      "last element: ('/ourdisk/hpc/ai2es/jroth/data/labeled/bronx_allsites/dry/NYSDOT_uyomtjhwsay_2022-02-02-04-31-12.jpg', 0)\n",
      "Number of elements in paths0: 161\n",
      "last element: /ourdisk/hpc/ai2es/jroth/data/labeled/bronx_allsites/dry/NYSDOT_uyomtjhwsay_2022-02-02-04-31-12.jpg\n",
      "Number of elements in lbl_topk0: 161\n",
      "last element: 0\n",
      "loader0 number of samples: 915\n",
      "last element: ('/ourdisk/hpc/ai2es/datasets/DOT/Skyline_6464/20220126/I_87_at_Interchange_3_(Yonkers_Mile_Square_Road)__Northbound__Skyline_6464_2022-01-26-21:30:19.jpg', 0)\n",
      "Number of unlabeled instances to remove: 161\n",
      "loader_unlbl0 number of samples: 3066\n"
     ]
    }
   ],
   "source": [
    "k = int(len(loader_unlbl0.dataset) * 0.05)\n",
    "cotrain(loader_train0, loader_train1, loader_unlbl0, loader_unlbl1, model0, model1, k, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "915\n",
      "915\n",
      "('/ourdisk/hpc/ai2es/datasets/DOT/Skyline_6464/20220126/I_87_at_Interchange_3_(Yonkers_Mile_Square_Road)__Northbound__Skyline_6464_2022-01-26-21:30:19.jpg', 0)\n",
      "('/ourdisk/hpc/ai2es/jroth/data/labeled/bronx_allsites/dry/NYSDOT_uyomtjhwsay_2022-02-02-04-31-12.jpg', 0)\n",
      "3066\n",
      "3066\n",
      "('/ourdisk/hpc/ai2es/jroth/data/labeled/bronx_allsites/wet/NYSDOT_m4er5dez4ab_2022-02-08-02-41-03.jpg', 2)\n",
      "('/ourdisk/hpc/ai2es/datasets/DOT/Skyline_6464/20220208/I_87_at_Interchange_3_(Yonkers_Mile_Square_Road)__Northbound__Skyline_6464_2022-02-08-02:40:29.jpg', 2)\n"
     ]
    }
   ],
   "source": [
    "# once again, many sanity checks\n",
    "print(len(loader_train0.dataset))\n",
    "print(len(loader_train1.dataset))\n",
    "\n",
    "print(loader_train0.dataset.samples[-1])\n",
    "print(loader_train1.dataset.samples[-1])\n",
    "\n",
    "print(len(loader_unlbl0.dataset))\n",
    "print(len(loader_unlbl1.dataset))\n",
    "\n",
    "print(loader_unlbl0.dataset.samples[1])\n",
    "print(loader_unlbl1.dataset.samples[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 64\n",
      "loss: 7.275452 [   64 /  3443]\n",
      "loss: 7.242462 [  128 /  3443]\n",
      "loss: 7.271658 [  192 /  3443]\n",
      "Batch size: 96\n",
      "loss: 6.953737 [   96 /  3443]\n",
      "loss: 6.971457 [  192 /  3443]\n",
      "loss: 6.974708 [  288 /  3443]\n",
      "Batch size: 128\n",
      "loss: 7.107782 [  128 /  3443]\n",
      "loss: 7.115186 [  256 /  3443]\n",
      "loss: 7.094270 [  384 /  3443]\n",
      "Batch size: 160\n",
      "loss: 7.628152 [  160 /  3443]\n",
      "loss: 7.727857 [  320 /  3443]\n",
      "loss: 7.776735 [  480 /  3443]\n",
      "Batch size: 192\n",
      "loss: 6.342412 [  192 /  3443]\n",
      "loss: 6.226246 [  384 /  3443]\n",
      "loss: 6.290689 [  576 /  3443]\n",
      "Batch size: 224\n",
      "loss: 6.562408 [  224 /  3443]\n",
      "loss: 6.534523 [  448 /  3443]\n",
      "loss: 6.540992 [  672 /  3443]\n",
      "Batch size: 256\n",
      "loss: 7.188202 [  256 /  3443]\n",
      "loss: 7.134872 [  512 /  3443]\n",
      "loss: 7.188458 [  768 /  3443]\n",
      "Batch size: 288\n",
      "loss: 7.491960 [  288 /  3443]\n",
      "loss: 7.488218 [  576 /  3443]\n",
      "loss: 7.497365 [  864 /  3443]\n",
      "Batch size: 320\n",
      "loss: 7.076617 [  320 /  3443]\n",
      "loss: 7.073470 [  640 /  3443]\n",
      "loss: 7.090246 [  960 /  3443]\n",
      "Batch size: 352\n",
      "loss: 6.587214 [  352 /  3443]\n",
      "loss: 6.569088 [  704 /  3443]\n",
      "loss: 6.574346 [ 1056 /  3443]\n",
      "Batch size: 384\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 31.74 GiB total capacity; 29.09 GiB already allocated; 120.88 MiB free; 30.41 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m loader_batch0 \u001b[38;5;241m=\u001b[39m DataLoader(data_train0, b, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mb\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloader_batch0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 10\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(loader, model, loss_fn, optimizer, device)\u001b[0m\n\u001b[1;32m      7\u001b[0m X, y \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Compute prediction error\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(pred, y)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n",
      "File \u001b[0;32m/home/jroth/.conda/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/home/jroth/.conda/envs/torch/lib/python3.10/site-packages/torchvision/models/resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/jroth/.conda/envs/torch/lib/python3.10/site-packages/torchvision/models/resnet.py:275\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    273\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(x)\n\u001b[1;32m    274\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)\n\u001b[0;32m--> 275\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer4(x)\n\u001b[1;32m    278\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x)\n",
      "File \u001b[0;32m/home/jroth/.conda/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/home/jroth/.conda/envs/torch/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/home/jroth/.conda/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/home/jroth/.conda/envs/torch/lib/python3.10/site-packages/torchvision/models/resnet.py:155\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    152\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n\u001b[1;32m    154\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(out)\n\u001b[0;32m--> 155\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m     identity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample(x)\n",
      "File \u001b[0;32m/home/jroth/.conda/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/home/jroth/.conda/envs/torch/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/jroth/.conda/envs/torch/lib/python3.10/site-packages/torch/nn/functional.py:2450\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m   2448\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2450\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2451\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[1;32m   2452\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 31.74 GiB total capacity; 29.09 GiB already allocated; 120.88 MiB free; 30.41 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "for b in range(64, 512 + 1, 32):\n",
    "    model0 = resnet50().to(device)\n",
    "    loader_batch0 = DataLoader(data_train0, b, shuffle=False)\n",
    "    print(f\"Batch size: {b}\")\n",
    "    train(loader_batch0, model0, loss_fn, optimizer0, device)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
